# src/analysis.py
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from tabulate import tabulate
from scipy.stats import ttest_rel
from sklearn.manifold import TSNE
from sklearn.model_selection import KFold
from torch_geometric.utils import degree
from .models import GNN, FinalGNN
from .engine import estimate_cate_gnn, train_gnn_model

def run_error_analysis(results: dict, edge_index, true_tau, config: dict):
    plt.figure(figsize=(12, 5))
    node_degrees = degree(edge_index[0], num_nodes=len(true_tau)).numpy()
    
    ax1 = plt.subplot(1, 2, 1)
    errors_ablation = (results['ablation_preds'] - true_tau).pow(2).numpy()
    ax1.scatter(node_degrees, errors_ablation, alpha=0.3, label='Squared Error')
    ax1.set_title("Error Analysis: Ablation (GNN+Linear)")
    ax1.set_xlabel("Node Degree"); ax1.set_ylabel("Squared CATE Error")
    ax1.set_ylim(bottom=0)

    ax2 = plt.subplot(1, 2, 2)
    errors_graphdml = (results['graphdml_preds'] - true_tau).pow(2).numpy()
    ax2.scatter(node_degrees, errors_graphdml, alpha=0.3, c='salmon')
    ax2.set_title("Error Analysis: Graph R-Learner (GNN+GNN)")
    ax2.set_xlabel("Node Degree"); ax2.set_ylabel("Squared CATE Error")
    ax2.set_ylim(bottom=0)

    plt.tight_layout()
    plot_filename = f"analysis_error_vs_degree_{config['name']}.png"
    plt.savefig(plot_filename)
    print(f"  -> Error analysis plot saved to {plot_filename}")
    return plt.gcf()

def run_tsne_visualization(X, edge_index, true_tau, config: dict):
    model = FinalGNN(X.shape[1])
    dummy_y = torch.randn_like(true_tau); dummy_t = torch.randn_like(true_tau)
    estimate_cate_gnn(dummy_y, dummy_t, X, edge_index, model_obj=model)
    
    with torch.no_grad():
        embeddings = F.relu(model.gnn.convs[0](X, edge_index))
    
    tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', max_iter=1000, random_state=42)
    tsne_results = tsne.fit_transform(embeddings.numpy())
    
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(tsne_results[:,0], tsne_results[:,1], c=true_tau.numpy(), cmap='viridis', alpha=0.7)
    plt.colorbar(scatter, label='True CATE Value')
    plt.title(f"t-SNE of FinalGNN Embeddings ({config['name']})")
    plt.xlabel("t-SNE Dimension 1"); plt.ylabel("t-SNE Dimension 2")
    plot_filename = f"analysis_tsne_{config['name']}.png"
    plt.savefig(plot_filename)
    print(f"  -> t-SNE visualization saved to {plot_filename}")
    return plt.gcf()

def run_two_model_test(baseline_mse: float, graphdml_mse: float, p_value: float):
    print("\n--- Practitioner's Diagnostic (Two-Model Test) ---")
    significant = p_value < 0.05
    large_effect = baseline_mse / (graphdml_mse + 1e-9) > 2.0
    if significant and large_effect:
        print("Result: POSITIVE. The Graph R-Learner significantly and substantially outperforms the blind baseline.")
        print("This is strong evidence for the presence of network-driven causal heterogeneity.")
        return "POSITIVE"
    else:
        print("Result: NEGATIVE. No strong evidence that a graph-aware model is necessary.")
        return "NEGATIVE"

def generate_report(results_over_seeds: dict, config: dict):
    headers = ["Method", "Mean MSE", "Std Dev MSE"]
    table, means, stds = [], {}, {}
    # Use a fixed order for the table
    method_order = ['baseline_mse', 'ablation_mse', 'sanity_check_mse', 'tlearner_mse', 'graphdml_mse']
    for key in method_order:
        if key in results_over_seeds:
            mses = results_over_seeds[key]
            means[key] = np.mean(mses)
            stds[key] = np.std(mses)
            table.append([key.replace('_mse','').upper(), f"{means[key]:.4f}", f"{stds[key]:.4f}"])
            
    config_str = f"Config: {config['name']}"
    report_str = f"\n\n{'='*70}\nRESULTS FOR {config_str}\n{'='*70}\n"
    report_str += tabulate(table, headers=headers, tablefmt="grid") + "\n"
    
    report_str += "\n--- Significance Tests (vs. Graph R-Learner) ---\n"
    _, p_abl = ttest_rel(results_over_seeds['graphdml_mse'], results_over_seeds['ablation_mse'])
    report_str += f"vs. Ablation (GNN+Lin): p-value = {p_abl:.2e} {'(Significant)' if p_abl < 0.05 else ''}\n"
    _, p_san = ttest_rel(results_over_seeds['graphdml_mse'], results_over_seeds['sanity_check_mse'])
    report_str += f"vs. Sanity (MLP+GNN):  p-value = {p_san:.2e} {'(Significant)' if p_san < 0.05 else ''}\n"
    if 'tlearner_mse' in results_over_seeds:
        _, p_tlearner = ttest_rel(results_over_seeds['graphdml_mse'], results_over_seeds['tlearner_mse'])
        report_str += f"vs. GNN T-Learner:    p-value = {p_tlearner:.2e} {'(Significant)' if p_tlearner < 0.05 else ''}\n"

    print(report_str) # Print to console
    
    # Plotting
    plot_labels = [
        'Baseline\n(MLP+Lin)', 'Ablation\n(GNN+Lin)', 'Sanity Check\n(MLP+GNN)',
        'GNN T-Learner\n(External)', 'Graph R-Learner\n(GNN+GNN)'
    ]
    mean_mses = [means[k] for k in method_order]
    std_devs = [stds[k] for k in method_order]
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(plot_labels, mean_mses, yerr=std_devs, capsize=5, color='skyblue')
    bars[-1].set_color('salmon') # Highlight Graph R-Learner
    plt.ylabel("Mean Squared Error in CATE Estimation (Lower is Better)")
    plt.title(f"End-to-End Graph Awareness is Necessary\n({config['name']})")
    plt.xticks(rotation=15, ha="right")
    plt.tight_layout()
    plot_filename = f"results_{config['name']}.png"
    plt.savefig(plot_filename)
    print(f"\nPlot saved to {plot_filename}")

    return means['baseline_mse'], means['graphdml_mse'], p_abl, plt.gcf(), report_str

def run_hub_vs_periphery_analysis(results: dict, edge_index, true_tau, config: dict):
    """
    Dissects the nuisance bottleneck on the BA graph to prove the hypothesis.
    """
    if config['data_params']['graph_type'] != 'ba':
        print("  -> Skipping Hub vs. Periphery analysis (not a BA graph).")
        return

    node_degrees = degree(edge_index[0], num_nodes=len(true_tau))
    
    # Define hubs and periphery based on degree quantiles
    hub_threshold = torch.quantile(node_degrees.float(), 0.9)
    periphery_threshold = torch.quantile(node_degrees.float(), 0.5)
    
    hub_mask = node_degrees >= hub_threshold
    periphery_mask = node_degrees <= periphery_threshold
    
    # Calculate errors for SANITY_CHECK (MLP+GNN)
    errors_sanity = (results['sanity_check_preds'] - true_tau).pow(2)
    mse_sanity_hubs = errors_sanity[hub_mask].mean().item()
    mse_sanity_periphery = errors_sanity[periphery_mask].mean().item()
    
    # Calculate errors for GRAPHDML (GNN+GNN)
    errors_graphdml = (results['graphdml_preds'] - true_tau).pow(2)
    mse_graphdml_hubs = errors_graphdml[hub_mask].mean().item()
    mse_graphdml_periphery = errors_graphdml[periphery_mask].mean().item()
    
    # --- Create the plot ---
    labels = ['Hubs (Top 10%)', 'Periphery (Bottom 50%)']
    sanity_mses = [mse_sanity_hubs, mse_sanity_periphery]
    graphdml_mses = [mse_graphdml_hubs, mse_graphdml_periphery]
    
    x = np.arange(len(labels))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, sanity_mses, width, label='Sanity Check (MLP+GNN)', color='khaki')
    rects2 = ax.bar(x + width/2, graphdml_mses, width, label='Graph R-Learner (GNN+GNN)', color='salmon')
    
    ax.set_ylabel('Mean Squared CATE Error')
    ax.set_title('Nuisance Bottleneck Analysis on BarabÃ¡si-Albert Graph')
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()
    fig.tight_layout()
    
    plot_filename = f"analysis_hub_vs_periphery_{config['name']}.png"
    plt.savefig(plot_filename)
    print(f"  -> Hub vs. Periphery analysis plot saved to {plot_filename}")
    return plt.gcf()
# src/data.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import GCNConv
from torch_geometric.utils import erdos_renyi_graph, stochastic_blockmodel_graph, barabasi_albert_graph
from torch_geometric.datasets import Planetoid

# We need a GNN definition here for the simulation, so we define a simple one.
# This avoids a circular import from src.models.
class SimGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.conv1 = GCNConv(in_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, out_dim)
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)


def simulate_data(seed=42, n=1000, d=10, graph_type='ba', cate_type='simple_h', 
                  real_data_name=None, noise_level=0.5): # <-- Added 'noise_level'
    """
    Simulates a world with configurable graph topology, CATE function, and noise level.
    Can operate in fully-synthetic or semi-synthetic mode.
    """
    torch.manual_seed(seed); np.random.seed(seed)
    
    if real_data_name:
        try:
            # Note: A real dataset should be specified in your YAML for real_data_name
            # e.g., real_data_name: 'Cora'
            dataset = Planetoid(root='/tmp/' + real_data_name, name=real_data_name)
            real_data = dataset[0]
            X, edge_index = real_data.x, real_data.edge_index
            n, d = X.shape
        except Exception as e:
            print(f"ERROR: Could not load real dataset '{real_data_name}'. {e}")
            return None
    else:
        X = torch.randn(n, d)
        if graph_type == 'er':
            # Assuming edge_prob is passed in data_params for ER graphs
            edge_prob = 0.05 
            edge_index = erdos_renyi_graph(n, edge_prob=edge_prob)
        elif graph_type == 'sbm':
            block_sizes = torch.tensor([n // 2, n - (n // 2)])
            edge_probs = torch.tensor([[0.1, 0.01], [0.01, 0.1]])
            edge_index = stochastic_blockmodel_graph(block_sizes, edge_probs)
        else:
            edge_index = barabasi_albert_graph(n, num_edges=4)
        
    confounder_gnn_1 = SimGNN(d, d, d)
    confounder_gnn_2 = SimGNN(d, d, d)
    with torch.no_grad():
        H_1hop = F.relu(confounder_gnn_1(X, edge_index))
        H_2hop = F.relu(confounder_gnn_2(H_1hop, edge_index))

    T_logits = 0.5 * X[:, 0] - 0.7 * H_1hop[:, 1] + 0.3 * torch.randn(n)
    T = (torch.sigmoid(T_logits) > 0.5).float().unsqueeze(1)

    if cate_type == 'local_x':
        true_causal_effect = 2.0 + 1.5 * torch.sin(X[:, 0])
    elif cate_type == 'higher_order':
        true_causal_effect = 2.0 + 1.5 * torch.sin(H_2hop[:, 0])
    elif cate_type == 'interaction':
        true_causal_effect = 2.0 + 1.0 * (H_1hop[:, 0] * X[:, 1])
    else:
        true_causal_effect = 2.0 + 1.5 * torch.sin(H_1hop[:, 0])
    
    # --- The critical change is here ---
    Y = (H_1hop[:, 0] + 0.5 * X[:, 1] + noise_level * torch.randn(n) + 
         T.squeeze() * true_causal_effect)
    
    return X, T, Y.unsqueeze(1), edge_index, true_causal_effect
# src/engine.py
import torch
import torch.nn.functional as F
import torch.nn as nn
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from .models import MLP, GNN, FinalGNN, T_Learner_GNN
import numpy as np 

# --- Training Utilities (Unchanged) ---
def train_model(model, X, y, train_idx, is_binary=False, **training_kwargs):
    lr = training_kwargs.get('lr', 1e-3); 
    epochs = training_kwargs.get('nuisance_epochs', 150)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.BCEWithLogitsLoss() if is_binary else nn.MSELoss()
    for _ in range(epochs): 
        model.train(); 
        opt.zero_grad(); 
        pred = model(X).squeeze(); 
        loss = loss_fn(pred[train_idx], y[train_idx]); 
        loss.backward(); 
        opt.step()
    return model

def train_gnn_model(model, X, y, edge_index, train_idx, is_binary=False, T=None, **training_kwargs):
    lr = training_kwargs.get('lr', 1e-3); epochs = training_kwargs.get('nuisance_epochs', 150)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.BCEWithLogitsLoss() if is_binary else nn.MSELoss()
    for _ in range(epochs):
        model.train(); opt.zero_grad()
        if T is not None: pred = model(X, T, edge_index).squeeze()
        else: pred = model(X, edge_index).squeeze()
        loss = loss_fn(pred[train_idx], y[train_idx]); loss.backward(); opt.step()
    return model

# --- Nuisance & CATE Estimators (Now fully flexible) ---

def get_nuisance_predictions(X, T, Y, edge_index=None, use_gnn=False, folds=2, 
                             model_kwargs={}, training_kwargs={}):
    n, kf = len(X), KFold(n_splits=folds, shuffle=True, random_state=42)
    Y_hat, T_hat = torch.zeros(n), torch.zeros(n)
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(range(n))):
        train_idx = torch.tensor(train_idx, dtype=torch.long)
        val_idx = torch.tensor(val_idx, dtype=torch.long)
        if use_gnn:
            outcome_model = train_gnn_model(GNN(X.shape[1], **model_kwargs), X, Y.squeeze(), edge_index, train_idx, False, **training_kwargs)
            treat_model = train_gnn_model(GNN(X.shape[1], **model_kwargs), X, T.squeeze(), edge_index, train_idx, True, **training_kwargs)
            with torch.no_grad():
                outcome_model.eval(); treat_model.eval()
                Y_hat[val_idx] = outcome_model(X, edge_index).squeeze()[val_idx]
                T_hat[val_idx] = torch.sigmoid(treat_model(X, edge_index).squeeze())[val_idx]
        else: 
            outcome_model = train_model(MLP(X.shape[1], **model_kwargs), X, Y.squeeze(), train_idx, False, **training_kwargs)
            treat_model = train_model(MLP(X.shape[1], **model_kwargs), X, T.squeeze(), train_idx, True, **training_kwargs)
            with torch.no_grad():
                outcome_model.eval(); treat_model.eval()
                Y_hat[val_idx] = outcome_model(X[val_idx]).squeeze()
                T_hat[val_idx] = torch.sigmoid(treat_model(X[val_idx]).squeeze())
    return Y_hat, T_hat

def estimate_cate_linear(Y_res, T_res, X):
    X_np, T_res_np, Y_res_np = [t.detach().numpy() for t in [X, T_res, Y_res]]
    reg = Ridge(alpha=0.1); reg.fit(X_np * T_res_np[:, np.newaxis], Y_res_np)
    return torch.from_numpy(reg.predict(X_np))

def estimate_cate_gnn(Y_res, T_res, X, edge_index, model_obj=None, model_kwargs={}, training_kwargs={}):
    lr = training_kwargs.get('lr', 1e-3); epochs = training_kwargs.get('cate_epochs', 200)
    model = model_obj if model_obj is not None else FinalGNN(X.shape[1], **model_kwargs)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    for _ in range(epochs):
        model.train(); opt.zero_grad()
        tau_hat_gnn = model(X, edge_index).squeeze()
        predicted_y_res = T_res.squeeze() * tau_hat_gnn
        loss = F.mse_loss(predicted_y_res, Y_res.squeeze())
        loss.backward(); opt.step()
    with torch.no_grad(): return model(X, edge_index).squeeze()

def estimate_cate_tlearner_gnn(X, T, Y, edge_index, model_kwargs={}, training_kwargs={}):
    epochs = training_kwargs.get('nuisance_epochs', 150) 
    model = T_Learner_GNN(X.shape[1], **model_kwargs)
    train_idx = torch.arange(len(X))
    model = train_gnn_model(model, X, Y.squeeze(), edge_index, train_idx, False, T=T, epochs=epochs, **training_kwargs)
    with torch.no_grad():
        y1_hat = model(X, torch.ones_like(T), edge_index).squeeze()
        y0_hat = model(X, torch.zeros_like(T), edge_index).squeeze()
    return y1_hat - y0_hat
# src/models.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv

class MLP(nn.Module):
    def __init__(self, in_dim, hidden_dim=32, out_dim=1, num_layers=2):
        super().__init__()
        layers = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])
        layers.append(nn.Linear(hidden_dim, out_dim))
        self.net = nn.Sequential(*layers)
    def forward(self, x): return self.net(x)

class GNN(nn.Module):
    """ A generalized GNN that can be instantiated with different layer types (e.g., GCN, GAT). """
    def __init__(self, in_dim, hidden_dim=32, out_dim=1, num_layers=2, layer_type='gcn', heads=4):
        super().__init__()
        self.convs = nn.ModuleList()
        self.layer_type = layer_type.lower()
        
        GNNLayer = GCNConv
        if self.layer_type == 'gat':
            GNNLayer = GATConv
        
        # First layer
        in_channels_next = in_dim
        if self.layer_type == 'gat':
            self.convs.append(GNNLayer(in_channels_next, hidden_dim, heads=heads))
            in_channels_next = hidden_dim * heads
        else:
            self.convs.append(GNNLayer(in_channels_next, hidden_dim))
            in_channels_next = hidden_dim

        # Hidden layers
        for _ in range(num_layers - 2):
            self.convs.append(GNNLayer(in_channels_next, hidden_dim))
            in_channels_next = hidden_dim
            
        # Final layer
        if self.layer_type == 'gat':
            self.convs.append(GNNLayer(in_channels_next, out_dim, heads=1))
        else:
            self.convs.append(GNNLayer(in_channels_next, out_dim))

    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i < len(self.convs) - 1:
                x = F.relu(x)
        return x

class FinalGNN(nn.Module):
    """ The GNN model for the final-stage R-Learner, now architecturally flexible. """
    def __init__(self, in_dim, **model_kwargs):
        super().__init__()
        self.gnn = GNN(in_dim, **model_kwargs)
    def forward(self, x, edge_index): return self.gnn(x, edge_index)

class T_Learner_GNN(nn.Module):
    """ The GNN model for the T-Learner baseline, now architecturally flexible. """
    def __init__(self, in_dim, **model_kwargs):
        super().__init__()
        self.gnn = GNN(in_dim + 1, **model_kwargs)
    def forward(self, x, t, edge_index):
        xt = torch.cat([x, t], dim=1)
        return self.gnn(xt, edge_index)
