# run_experiments.py
"""
Main orchestrator for the Graph R-Learner paper experiments.

This script reads experiment configurations from YAML files, runs the full
multi-seed experimental suite for each configuration, generates all result
tables and plots, and logs everything to Weights & Biases for transparent
and reproducible research.

Usage:
    python run_experiments.py --config=configs/main_ba_simple_h.yaml
    python run_experiments.py --all  (to run all configs in the /configs dir)
"""

import yaml
import wandb
import torch
import os
import argparse
from typing import Dict
import matplotlib.pyplot as plt
# Import all the necessary components from your src package
from src.data import simulate_data
from src.engine import (
    get_nuisance_predictions,
    estimate_cate_linear,
    estimate_cate_gnn,
    estimate_cate_tlearner_gnn
)
from src.analysis import (
    generate_report,
    run_two_model_test,
    run_error_analysis,
    run_tsne_visualization,
    run_hub_vs_periphery_analysis
)

def run_single_experiment(seed: int, config: Dict) -> Dict:
    """
    Executes one full experimental run for a single seed and configuration.
    Returns a dictionary of all resulting MSEs and prediction tensors.
    """
    print(f"  Running Seed {seed+1}/{config['num_seeds']}...")
    
    # --- Unpack config parameters with clarity ---
    data_args = config.get('data_params', {})
    model_args = config.get('model_params', {})
    training_args = config.get('training_params', {})
    
    # --- Simulate Data ---
    sim_data = simulate_data(seed=seed, **data_args)
    if sim_data is None: return None
    X, T, Y, edge_index, true_tau = sim_data
    
    # --- R-Learner Nuisance Component ---
    # Pass model and training kwargs separately
    Y_hat_base, T_hat_base = get_nuisance_predictions(
        X, T, Y, use_gnn=False, 
        model_kwargs=model_args, 
        training_kwargs=training_args
    )
    Y_res_base, T_res_base = Y.squeeze() - Y_hat_base, T.squeeze() - T_hat_base
    
    Y_hat_graph, T_hat_graph = get_nuisance_predictions(
        X, T, Y, edge_index, use_gnn=True, 
        model_kwargs=model_args, 
        training_kwargs=training_args
    )
    Y_res_graph, T_res_graph = Y.squeeze() - Y_hat_graph, T.squeeze() - T_hat_graph
    
    # --- CATE Estimation Stage ---
    results = {}
    results['baseline_preds'] = estimate_cate_linear(Y_res_base, T_res_base, X)
    results['ablation_preds'] = estimate_cate_linear(Y_res_graph, T_res_graph, X)
    results['sanity_check_preds'] = estimate_cate_gnn(
        Y_res_base, T_res_base, X, edge_index, 
        model_kwargs=model_args, 
        training_kwargs=training_args
    )
    results['graphdml_preds'] = estimate_cate_gnn(
        Y_res_graph, T_res_graph, X, edge_index, 
        model_kwargs=model_args, 
        training_kwargs=training_args
    )
    
    # --- External T-Learner Baseline ---
    results['tlearner_preds'] = estimate_cate_tlearner_gnn(
        X, T, Y, edge_index, 
        model_kwargs=model_args, 
        training_kwargs=training_args
    )
    
    # --- Calculate MSEs ---
    for key, preds in list(results.items()):
        if '_preds' in key:
            results[key.replace('_preds', '_mse')] = torch.mean((preds - true_tau)**2).item()
            
    return results

def main(args):
    """ Main function to orchestrate the experimental suites. """
    
    if args.all:
        config_files = [os.path.join('configs', f) for f in os.listdir('configs') if f.endswith('.yaml')]
    else:
        config_files = [args.config]

    for config_path in config_files:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        # --- Start a new W&B Run for this configuration ---
        run = wandb.init(
            project="final-stage-bottleneck",
            name=config['name'],
            config=config,
            reinit=True
        )

        results_over_seeds = {
            'baseline_mse': [], 'ablation_mse': [], 'sanity_check_mse': [],
            'graphdml_mse': [], 'tlearner_mse': []
        }
        
        config_str = f"Config: {config['name']}"
        print(f"\n{'#'*70}\n# Starting Experiment Suite: {config_str}\n{'#'*70}")
        
        first_seed_preds = None
        for i in range(config['num_seeds']):
            seed_results = run_single_experiment(seed=i, config=config)
            if seed_results is None: continue
            
            if i == 0: first_seed_preds = seed_results
            
            per_seed_metrics = {f'seed_{i+1}_{k}': v for k, v in seed_results.items() if '_mse' in k}
            wandb.log(per_seed_metrics, step=i+1)
            
            for key in results_over_seeds:
                if key in seed_results:
                    results_over_seeds[key].append(seed_results[key])
        
        # --- Generate Final Report & Log to W&B ---
        baseline_mse, graphdml_mse, p_value, final_plot, report_str = generate_report(results_over_seeds, config)
        run.log({"final_results_plot": wandb.Image(final_plot)})
        run.summary['final_report'] = report_str
        plt.close(final_plot)

        # --- Run and Log Diagnostics ---
        diag_result = run_two_model_test(baseline_mse, graphdml_mse, p_value)
        run.summary['diagnostic_result'] = diag_result
        
        # --- Generate and Log Analysis Plots ---
        if first_seed_preds and config['data_params'].get('cate_type') != 'local_x':
            print("\n--- Generating and Logging Analysis Plots ---")
            data_args = {'seed': 0, **config['data_params']}
            sim_data = simulate_data(**data_args)
            if sim_data:
                X, _, _, edge_index, true_tau = sim_data
                
                error_plot = run_error_analysis(first_seed_preds, edge_index, true_tau, config)
                run.log({"error_analysis_plot": wandb.Image(error_plot)})
                plt.close(error_plot)
                
                if 'real_data_name' not in config['data_params'] or not config['data_params']['real_data_name']:
                     tsne_plot = run_tsne_visualization(X, edge_index, true_tau, config)
                     run.log({"tsne_plot": wandb.Image(tsne_plot)})
                     plt.close(tsne_plot)
                if config['name'] == 'Main_Result_BA_Graph':
                    print("\n--- Running Hub vs. Periphery Analysis ---")
                    hub_plot = run_hub_vs_periphery_analysis(first_seed_preds, edge_index, true_tau, config)
                    run.log({"hub_vs_periphery_plot": wandb.Image(hub_plot)})
                    plt.close(hub_plot)                 
        run.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Graph R-Learner paper experiments.")
    parser.add_argument('--config', type=str, help="Path to a single experiment config YAML file.")
    parser.add_argument('--all', action='store_true', help="Flag to run all experiments in the 'configs' directory.")
    
    args = parser.parse_args()

    if not args.config and not args.all:
        parser.error("Please specify a config file with --config or run all with --all.")
    
    main(args)
# run_sensitivity_analyses.py
import torch
import numpy as np
import matplotlib.pyplot as plt
import yaml
import wandb
from tqdm import tqdm
from typing import Dict, List
import torch.nn.functional as F

# Import all the necessary components from your src package
from src.data import simulate_data
from src.engine import get_nuisance_predictions, estimate_cate_linear, estimate_cate_gnn

# --- Add the GAT Model for the Architecture Sensitivity Test ---
try:
    from torch_geometric.nn import GATConv
    from src.models import GNN # Use the flexible GNN from your models
    GAT_ENABLED = True
except (ImportError, ModuleNotFoundError):
    print("Warning: GATConv could not be imported. Architecture sensitivity test will be skipped.")
    GAT_GNN = None
    GAT_ENABLED = False

# ============================================================
# Core Experiment Logic (Unchanged)
# ============================================================
def run_single_sensitivity_exp(seed: int, config: Dict) -> Dict:
    data_args = {'seed': seed, **config['data_params']}
    sim_data = simulate_data(**data_args)
    if sim_data is None: return {'ablation': float('nan'), 'graphdml': float('nan')}
    X, T, Y, edge_index, true_tau = sim_data
    
    Y_hat_graph, T_hat_graph = get_nuisance_predictions(
        X, T, Y, edge_index, use_gnn=True, 
        model_kwargs=config['model_params'], 
        training_kwargs=config['training_params']
    )
    Y_res_graph, T_res_graph = Y.squeeze() - Y_hat_graph, T.squeeze() - T_hat_graph
    
    tau_hat_ablation = estimate_cate_linear(Y_res_graph, T_res_graph, X)
    mse_ablation = torch.mean((tau_hat_ablation - true_tau)**2).item()
    
    tau_hat_full = estimate_cate_gnn(
        Y_res_graph, T_res_graph, X, edge_index, 
        model_kwargs=config['model_params'], 
        training_kwargs=config['training_params']
    )
    mse_full = torch.mean((tau_hat_full - true_tau)**2).item()
    
    return {'ablation': mse_ablation, 'graphdml': mse_full}


# ============================================================
# Analysis 1: Architecture Sensitivity (Now with W&B)
# ============================================================
def run_arch_sensitivity(config: Dict, num_seeds: int):
    if not GAT_ENABLED: return

    run = wandb.init(project="final-stage-bottleneck-appendix", name="Architecture_Sensitivity", config=config)
    print("\n--- Running Architecture Sensitivity Analysis (GCN vs. GAT) ---")
    results_gcn, results_gat = {'ablation': [], 'graphdml': []}, {'ablation': [], 'graphdml': []}

    for i in tqdm(range(num_seeds), desc="Arch Sensitivity Seeds"):
        config_gcn = yaml.safe_load(yaml.dump(config))
        config_gcn['model_params']['layer_type'] = 'gcn'
        res_gcn = run_single_sensitivity_exp(seed=i, config=config_gcn)
        for key in res_gcn: results_gcn[key].append(res_gcn[key])
        
        config_gat = yaml.safe_load(yaml.dump(config))
        config_gat['model_params']['layer_type'] = 'gat'
        res_gat = run_single_sensitivity_exp(seed=i, config=config_gat)
        for key in res_gat: results_gat[key].append(res_gat[key])

    labels = ['Ablation (GNN+Lin)', 'Graph R-Learner (GNN+GNN)']
    gcn_means = [np.mean(results_gcn['ablation']), np.mean(results_gcn['graphdml'])]
    gat_means = [np.mean(results_gat['ablation']), np.mean(results_gat['graphdml'])]
    
    x = np.arange(len(labels)); width = 0.35
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(x - width/2, gcn_means, width, label='GCN (Baseline Arch)', color='skyblue')
    ax.bar(x + width/2, gat_means, width, label='GAT (Alternative Arch)', color='salmon')
    ax.set_ylabel('Mean Squared CATE Error'); ax.set_title('Architecture Sensitivity: GCN vs. GAT')
    ax.set_xticks(x); ax.set_xticklabels(labels); ax.legend(); fig.tight_layout()
    
    run.log({"architecture_sensitivity_plot": wandb.Image(fig)})
    plt.savefig("sensitivity_architecture.png"); plt.close()
    print("  -> Architecture sensitivity plot saved and logged.")
    run.finish()

# ============================================================
# Analysis 2 & 3: Noise and Sample Size Sweeps (Now with W&B)
# ============================================================
def run_parameter_sweep(config: Dict, param_name: str, param_values: List, num_seeds: int):
    run = wandb.init(project="final-stage-bottleneck-appendix", name=f"{param_name.title()}_Sweep", config={'param': param_name, 'values': param_values})
    print(f"\n--- Running Sensitivity Analysis for '{param_name}' ---")
    
    all_results = {'ablation': [], 'graphdml': []}

    for value in tqdm(param_values, desc=f"Sweeping {param_name}"):
        temp_config = yaml.safe_load(yaml.dump(config))
        if param_name in temp_config['data_params']: temp_config['data_params'][param_name] = value
        else: temp_config[param_name] = value
        
        seed_results = {'ablation': [], 'graphdml': []}
        for i in range(num_seeds):
            res = run_single_sensitivity_exp(seed=i, config=temp_config)
            for key in res: seed_results[key].append(res[key])
        
        # Log the mean result for this parameter value to W&B
        wandb.log({
            f'mean_mse_ablation': np.mean(seed_results['ablation']),
            f'mean_mse_graphdml': np.mean(seed_results['graphdml']),
            param_name: value
        }, step=int(value) if isinstance(value, (int, float)) else None)

        for key in all_results:
            all_results[key].append(np.mean(seed_results[key]))
            
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(param_values, all_results['ablation'], marker='o', linestyle='--', label='Ablation (GNN+Lin)')
    ax.plot(param_values, all_results['graphdml'], marker='s', linestyle='-', label='Graph R-Learner (GNN+GNN)')
    ax.set_xlabel(param_name.replace('_', ' ').title()); ax.set_ylabel('Mean Squared CATE Error')
    ax.set_title(f'Model Performance vs. {param_name.replace("_", " ").title()}')
    ax.legend(); ax.grid(True, which='both', linestyle='--', linewidth=0.5); fig.tight_layout()

    run.log({f"{param_name}_sweep_plot": wandb.Image(fig)})
    plt.savefig(f"sensitivity_{param_name}.png"); plt.close()
    print(f"  -> {param_name} sensitivity plot saved and logged.")
    run.finish()

# ============================================================
# Main Orchestrator for Sensitivity Analyses
# ============================================================
if __name__ == "__main__":
    with open('configs/main_ba_simple_h.yaml', 'r') as f:
        base_config = yaml.safe_load(f)

    NUM_SEEDS_SENSITIVITY = 5

    # --- Analysis 1: Architecture Sensitivity ---
    run_arch_sensitivity(config=base_config, num_seeds=NUM_SEEDS_SENSITIVITY)

    # --- Analysis 2: Signal-to-Noise Ratio ---
    run_parameter_sweep(config=base_config, 
                        param_name='noise_level', 
                        param_values=[0.1, 0.5, 1.0, 2.0, 5.0], 
                        num_seeds=NUM_SEEDS_SENSITIVITY)

    # --- Analysis 3: Sample Efficiency ---
    run_parameter_sweep(config=base_config, 
                        param_name='n', 
                        param_values=[200, 500, 1000, 2000, 5000], 
                        num_seeds=NUM_SEEDS_SENSITIVITY)
    
    print("\nAll sensitivity analyses complete and logged to W&B project 'final-stage-bottleneck-appendix'.")
